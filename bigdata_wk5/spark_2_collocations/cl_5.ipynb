{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext(conf=SparkConf().setAppName(\"Collocations\").setMaster(\"local\"))\n",
    "# stop_word_path = \"stop_words_en.txt\"\n",
    "# wiki_path = \"input1.txt\"\n",
    "# bigram_filter_threshold = 2\n",
    "# print_top_threshold = 5\n",
    "wiki_path = \"/data/wiki/en_articles_part\"\n",
    "stop_word_path = \"/datasets/stop_words_en.txt\"\n",
    "bigram_filter_threshold = 500\n",
    "print_top_threshold = 39\n",
    "\n",
    "\n",
    "def load_stop_words(path):\n",
    "    stop_words = []\n",
    "    input_file = open(path, \"r\")\n",
    "    for word in input_file:\n",
    "        stop_words.append(word.rstrip(\"\\n\"))\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def parse_article(line):\n",
    "    try:\n",
    "        article_id, text = line.rstrip().split('\\t', 1)\n",
    "        text = re.sub(\"^\\W+|\\W+$\", \"\", text).lower()\n",
    "        words = re.split(\"\\W*\\s+\\W*\", text)\n",
    "        return words\n",
    "    except ValueError as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def filter_stop_words(words):\n",
    "    output_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_word_bcast.value:\n",
    "            output_words.append(word)\n",
    "    return output_words\n",
    "\n",
    "\n",
    "def get_bigrams(input_words):\n",
    "    bigrams = []\n",
    "    for word in input_words:\n",
    "        if input_words.index(word) < len(input_words) - 1:\n",
    "            bigram = word + \"_\" + input_words[input_words.index(word) + 1]\n",
    "            bigrams.append((bigram, 1))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def get_word_tuples(words):\n",
    "    word_tuples_list = []\n",
    "    for word in words:\n",
    "        word_tuples_list.append((word, 1))\n",
    "    return word_tuples_list\n",
    "\n",
    "\n",
    "def get_word_probs(input_tuple):\n",
    "    word, count = input_tuple\n",
    "    prob = float(count) / float(total_count_bcast.value)\n",
    "    return word, prob\n",
    "\n",
    "\n",
    "def get_bigram_probs(input_tuple):\n",
    "    bigram, count = input_tuple\n",
    "    prob = float(count) / float(total_bigram_count_bcast.value)\n",
    "    return bigram, prob\n",
    "\n",
    "\n",
    "def calc_npmi(input_tuple):\n",
    "    bigram, bigram_prob = input_tuple\n",
    "    bigram_words = bigram.split(\"_\")\n",
    "    word_1_prob = word_probs_bcast.value[bigram_words[0]]\n",
    "    word_2_prob = word_probs_bcast.value[bigram_words[1]]\n",
    "    pmi = math.log(bigram_prob / (word_1_prob * word_2_prob))\n",
    "    npmi = pmi / -math.log(bigram_prob)\n",
    "    return bigram, npmi\n",
    "\n",
    "\n",
    "stop_word_list = load_stop_words(stop_word_path)\n",
    "stop_word_bcast = sc.broadcast(stop_word_list)\n",
    "\n",
    "wiki = sc.textFile(wiki_path).map(parse_article)\n",
    "filtered_wiki = wiki.map(filter_stop_words)\n",
    "filtered_wiki.cache()\n",
    "\n",
    "word_tuples = filtered_wiki.flatMap(get_word_tuples)\n",
    "word_counts = word_tuples.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "word_count_tuples = wiki.flatMap(get_word_tuples)\n",
    "word_count_agg = word_count_tuples.reduceByKey(lambda x, y: x + y)\n",
    "dirty_word_count = word_counts.values().sum()\n",
    "print('Total dirty word count', str(dirty_word_count))\n",
    "total_word_count = word_counts.values().sum()\n",
    "print('Total clean word count', str(total_word_count))\n",
    "total_count_bcast = sc.broadcast(total_word_count)\n",
    "\n",
    "word_probabilities = word_counts.map(get_word_probs).collectAsMap()\n",
    "word_probs_bcast = sc.broadcast(word_probabilities)\n",
    "\n",
    "bigram_tuples = filtered_wiki.flatMap(get_bigrams)\n",
    "bigram_counts = bigram_tuples.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "bigram_count_tuples = wiki.flatMap(get_bigrams)\n",
    "bigram_count_agg = bigram_count_tuples.reduceByKey(lambda x, y: x + y)\n",
    "dirty_bigram_count = bigram_count_agg.values().sum()\n",
    "print('Total dirty bigram count', str(dirty_bigram_count))\n",
    "total_bigram_count = bigram_counts.values().sum()\n",
    "print('Total clean bigram count', str(total_bigram_count))\n",
    "total_bigram_count_bcast = sc.broadcast(total_bigram_count)\n",
    "\n",
    "filtered_bigrams = bigram_counts.filter(lambda x: x[1] >= bigram_filter_threshold)\n",
    "bigram_probs = filtered_bigrams.map(get_bigram_probs)\n",
    "bigram_npmis = bigram_probs.map(calc_npmi)\n",
    "bigram_npmis_sorted = bigram_npmis.sortBy(lambda x: -x[1])\n",
    "\n",
    "for bigram in bigram_npmis_sorted.collect()[:print_top_threshold]:\n",
    "    print(bigram[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
